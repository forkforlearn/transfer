<font color=#0099ff size=12 face="黑体">xhzyk等项目通用的一部分技术思路</font>



* ## 新旧资源的统一认证
    * ### 方案一：最前方放置nginx,反向代理到中间层；用node.js或者python做一个中间层；中间层之后是资源:
        > 简单来说就是前面是nginx，通过pass_proxy将请求路由到node.js或python写的身份识别用的中间层，身份识别后根据响应返回的结果决定如何设置用户cookie以及其他相关位置的用户信息，并且对接最后端的资源形成一条链路。
    * ### <font color="red">方案二：最前方放置openResty,直接通过lua对访问进行控制</font>
         > 这个方案比第一个方案要减少一个层，并且输出资源内容的时候，是由openResty的nginx那部分输出内容:)，出现问题概率更小。需要提前在内网建立好统一身份认证问题。
         
        * #### 安装
        ```nginx                
        openResty:
        #提前需要安装gcc python等，这些就不赘述了
        wget https://openresty.org/download/openresty-1.13.6.1.tar.gz
        ./configure --prefix=/nginx --with-luajit --without-http_redis2_module --with-http_iconv_module --with-http_postgres_module
        make
        make install
        luarocks:
        wget https://luarocks.org/releases/luarocks-2.4.3.tar.gz
        tar zfvx luarocks-2.4.3.tar.gz
        cd luarocks-2.4.3/
        ./configure  --with-lua=/nginx/luajit/  --lua-suffix=jit  --with-lua-include=/nginx/luajit/include/luajit-2.1
        make build && make install
        ```
        * #### 配置
        ```bash
        vim /nginx/nginx/conf/config.conf
        http {
            lua_package_path 'conf/?.lua;;';
            include       mime.types;
            default_type  application/octet-stream;
            access_log  logs/access.log  main;
            sendfile        on;
            keepalive_timeout  65;
            server {
                listen       80;
                server_name  172.16.100.146;
                charset utf-8;
                access_log  logs/host.access.log  main;
                location /search {
                  rewrite_by_lua_file conf/lua.lua;
                  proxy_set_header   X-Real-IP            $remote_addr;
                  proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
                  proxy_set_header   X-NginX-Proxy    true;
                  proxy_set_header   Connection "";
                  proxy_http_version 1.1;
                  proxy_pass http://172.16.201.138/search;
                }
                location /tianjinwe/ {
                  rewrite_by_lua_file conf/lua.lua;
                  proxy_set_header   X-Real-IP            $remote_addr;
                  proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
                  proxy_set_header   X-NginX-Proxy    true;
                  proxy_set_header   Connection "";
                  proxy_http_version 1.1;
                  proxy_pass http://172.16.201.138/;
                }
            }
        }
        需要根据局实际情况确定Host是否传递，proxy_set_header   Host  $http_host;
        ```
        * #### 身份识别代码
        ```lua
            -- conf/lua.lua

            local rocks = require "luarocks.loader"
            local http = require("resty.http")
            local md5 = require "md5"
            local json = require "cjson"
            local _var1 = ngx.var.cookie_token
            if nil == _var1 and nil then
              ngx.log(ngx.ERR, "frontend:", _var1)
              ngx.header.content_type = "application/json; charset=utf-8"
              --ngx.say(json.encode(ngx.req.get_headers()))
              ngx.exit(404)
            -- ngx.exit(401)
            else
              ngx.log(ngx.INFO, "frontend:", _var1)
              -- 将来需要考虑长连接的其它问题
              local httpc = http.new()
              local headers = {
                -- ["Authorization"] = "Bearer 6e1ce2f2-81f2-4a19-bdbf-463c41860773"
                ["Authorization"] = cookie_token
              }
              -- 连接到后面的统一认证服务器
              local res, err = httpc:request_uri("http://127.0.0.1:11111/hexiwangxin/public/role/list/0/1", { method = "GET",
                headers = headers
              })
              ngx.log(ngx.ERR, "status:", res.status)
              -- 代码级错误处理以及对于返回后的code咳咳做进一步的筛选及处理
              if 200 ~= res.status then
                ngx.exit(res.status)
              end
            end
            --ngx.header.content_type = "application/json; charset=utf-8"
            --ngx.say(_var1)
            --ngx.say(json.encode(ngx.req.get_headers()))
        
        ```



* ## 搜索 
    * ### 方案：采用ElasticSearch系列提供文章检索
        > 通过监控生成的静态文件目录，或者在静态化期间将待搜索内容（标题，内容等）及相关联的资源（如外网访问的链接）存入ElasticSearch。ElasticSearch本身提供二进制包，可在 https://www.elastic.co/cn/ 进行下载。

        > 如果是centos这类系统，建议先做以下事情(ubuntu也差不多):
        
        ```bash
            vim etc/sysctrl.conf
            fs.file-max = 1000000
            vm.max_map_count=262144
            vm.swappiness = 1

            vim /etc/security/limits.conf
            * soft nofile 65536
            * hard nofile 131072

            vi /etc/security/limits.d/90-nproc.conf
            *          soft    nproc     2048        
        ```
        
        > 下面给出测试时的配置（个人喜好三主三数据这样的）        
        
        * #### ElasticSearch配置
        ```nginx
        #集群名
        cluster.name: search
        #节点名称
        node.name: node1
        #是否为主节点
        node.master: true
        #是否为数据节点
        node.data: false
        #绑定地址
        network.host: 172.16.100.31
        #该集群内的机器有男鞋
        discovery.zen.ping.unicast.hosts: ["172.16.100.30", "172.16.130.31", "172.16.130.32"]
        #一个节点需要看到的具有master节点资格的最小数量，然后才能在集群中做操作。官方的推荐值是(N/2)+1
        discovery.zen.minimum_master_nodes: 2
        #设置集群中N个节点启动后进行数据恢复
        gateway.recover_after_nodes: 3
        #其他的可在网上进行查找
        bootstrap.memory_lock: false
        bootstrap.system_call_filter: false
        ```
        * #### logstash配置
        ```nginx
        input {
          http{
              #输入端所监听的地址、端口
              host => "0.0.0.0"
              port => 8111
              #内容解码类型、内容字符集
              codec => json {
                charset => ["UTF-8"]
              }
              #线程数
              threads => 4
              #用户名密码
              user => tran
              password => zerotest
              #是否使用ssl
              ssl => false
          }
        }
        output {
          #可同时向标准、es输出
          # 标准输出解码方式
          stdout {  codec => rubydebug }
          elasticsearch {
                 #es所在的地址以及端口
                 hosts => ["http://127.0.0.1:9200"]
                 #希望es做什么动作（这个字段在后面写到了程序里，可以由程序来控制增加、更新、删除）
                 action => "%{action}"
                 #用户名密码
                 user => elastic
                 password => changeme
                 #文档唯一ID
                 document_id => "%{hashid}"
                 #此文档要使用哪个索引
                 index => "%{indexname}"
                 #此文档要使用哪个文档类型（可以理解document_type是表、index是库。。。不要完全照搬rl类数据库的原始概念）
                 document_type => mixdoc
                 doc_as_upsert => true
          }
        }
        
        ```

        > 下一步动作之前开始安装ik，x-pack等插件，ik是中文分词，需要安装到es下，x-pack是安全、监控等功能的官方插件（收费），是选择性使用的东西，用了后有漂亮的监控界面，不用也不影响系统正常使用，就是监控相应的情况麻烦。当然，有那啥。。。开始建立索引和文档类型(在装好ik的情况下,另外es系统缺省用户名密码为elastic:changeme)，以索引名tianjinwe举例
        
        ```bash
        #先建立tianjinwe_v1这个索引,参数都很好理解就不多说了
        curl -u elastic:changeme -XPUT 'http://172.16.100.30:9200/tianjinwe_v1' -d'
        {
          "settings": {
            "number_of_shards": 10,
            "number_of_replicas": 1,
            "index": {
              "analysis": {
                "analyzer": {
                  "by_smart": {
                    "type": "custom",
                    "tokenizer": "ik_smart",
                    "filter": ["by_tfr","by_sfr"],
                    "char_filter": ["by_cfr"]
                  },
                  "by_max_word": {
                    "type": "custom",
                    "tokenizer": "ik_max_word",
                    "filter": ["by_tfr","by_sfr"],
                    "char_filter": ["by_cfr"]
                  }
                },
                "filter": {
                  "by_tfr": {
                    "type": "stop",
                    "stopwords": [" "]
                  },
                  "by_sfr": {
                    "type": "synonym",
                    "synonyms_path": "analysis/synonyms.txt"
                  }
                },
                "char_filter": {
                  "by_cfr": {
                    "type": "mapping",
                    "mappings": ["| => |"]
                  }
                }
              }
            }    
          }
        }'        
        ```
        
        > 建立文档映射（document_type类似表及其内部字段）
        
        ```
        curl  -u elastic:changeme  -XPUT 'http://172.16.100.30:9200/tianjinwe_v1/_mapping/mixdoc' -d'
        {
          "properties": {
            "hashid" : {
              "type": "keyword"
            },
            "title": {
              "type": "text",
              "index": true,
              "analyzer": "by_max_word",
              "search_analyzer": "by_smart"
            },
            "content": {
              "type": "text",
              "index": true,
              "analyzer": "by_max_word",
              "search_analyzer": "by_smart"
            },
            "keys": {
              "type": "keyword"
            },
            "date": {
              "type": "date"
            },
            "description": {
              "type": "text"
            },
            "pics": {
              "type": "text"
            },
            "link": {
              "type": "text"
            },
            "author": {
              "type": "keyword"
            },
            "mark_int": {
              "type": "long"
            },
            "mark_text": {
              "type": "text"
            },    
            "area_location": {
              "type": "geo_point"
            },
            "area_shape": {
              "type": "geo_shape"
            },
            "other_backup1": {
              "type": "text",
              "index": false
            },
            "other_backup2": {
              "type": "text",
              "index": false
            }          
          }
        }'        
        ```
        
        > 通过别名方式建立真正的tianjinwe这个doucment_type(这么做的好处是因为es不支持为document_type增加字段，如果增加就会全量索引，为了避免更迭出现的问题，用别名的方式建立最好，这样可以做成类似链表的接口，改变指针即可)
        
        ```bash
        curl -u elastic:changeme -XPOST 172.16.100.30:9200/_aliases -d '
        {
            "actions": [
                { "add": {
                    "alias": "tianjinwe",
                    "index": "tianjinwe_v1"
                }}
            ]
        }'        
        ```

        > 静态化内容存入ES的方式有很多，如果是涉及到已经成型的发布系统，而发布系统本身没有对外提供相应的自动化数据接口，那么就我个人的习惯会根据历史数据量的多少、每天更新量的多少、本项目是否与其他项目混用同一级别的发布目录来进行不同方案的选择。因为本项目实质上内容不会太多，所以直接采用了监控目录的方法，方法很简单，使用linux内核的inotify机制（对系统内核有一点点要求，这个请自行查找），采用类似下面的python代码即可，届时只需稍微提高代码的容错性并实现故障的情况下自动重启（<font color="red">个人建议supervisor</font>），并要将打印部分的代码修改为连接kafka或者存入数据库，另外再写一个消费这些信息的程序将数据写入logstash或者直接写入elasticSearch即可，这两个部分的示意代码如下(py3比较好。至于py2.。。。):
        
        >  监控目录的示例程序是:
        
        ```python
        import os
        from  pyinotify import * # WatchManager, Notifier, ProcessEvent,IN_DELETE, IN_CREATE,IN_MODIFY,IN_CLOSE_WRITE,IN_CLOSE_NOWRITE
        class EventHandler(ProcessEvent):
            """事件处理"""
            max_queued_events.value = 99999
            def process_IN_CREATE(self, event):
                findIndex = event.name.find('index')
                findHtm = event.name.find('htm')
                if findIndex >= 0 and findHtm >= 0:
                    print("Create file: %s "  %   os.path.join(event.path,event.name))

            def process_IN_DELETE(self, event):
                findIndex = event.name.find('index')
                findHtm = event.name.find('htm')
                if findIndex >= 0 and findHtm >= 0:
                    print("Delete file: %s "  %   os.path.join(event.path,event.name))

            def process_IN_MODIFY(self, event):
                findIndex = event.name.find('index')
                findHtm = event.name.find('htm')
                if findIndex >= 0 and findHtm >= 0:
                    print("Modify file: %s "  %   os.path.join(event.path,event.name))

            def process_IN_CLOSE_WRITE(self, event):
                findIndex = event.name.find('index')
                findHtm = event.name.find('htm')
                if findIndex >= 0 and findHtm >= 0:
                    print("CLOSE WRITE file: %s "  %   os.path.join(event.path,event.name))

            def process_IN_NOMODIFY(self, event):
                findIndex = event.name.find('index')
                findHtm = event.name.find('htm')
                if findIndex >= 0 and findHtm >= 0:
                    print("CLOSE NOWRITE: %s "  %   os.path.join(event.path,event.name))

            def process_IN_Q_OVERFLOW(self, event):
                print('-_-',max_queued_events.value)
                max_queued_events.value *= 3

        def FSMonitor(path='.'):
            wm = WatchManager()
            mask = IN_DELETE | IN_CLOSE_WRITE | IN_CLOSE_NOWRITE
            notifier = Notifier(wm, EventHandler())
            wm.add_watch(path, mask,rec=True)
            print('now starting monitor %s'%(path))
            while True:
                try:
                    notifier.process_events()
                    if notifier.check_events():
                        notifier.read_events()
                except KeyboardInterrupt:
                    notifier.stop()
                    break

        if __name__ == "__main__":
            FSMonitor('/data/wwwroot/')        
        ```
        
        >  向logstash传递数据的示例程序(此示例程序用于搜索某个目录下的所有html文件，并解析出其标题、内容等项，并传递给logstash):
        
        ```python
        from bs4 import BeautifulSoup
        import json
        import os
        import os.path
        import hashlib
        import urllib3
        http = urllib3.PoolManager()
        logstash = 'http://172.16.130.52:8080'
        rootdir="/data/wwwroot/"
        _counter = 0
        for parent,dirnames,filenames in os.walk(rootdir):
            for filename in filenames:
                if filename.find('.html') < 0:
                    continue
                completeFileName = os.path.join(parent,filename)
                hash = hashlib.sha256()
                hash.update(completeFileName.encode('utf-8'))
                #为每一个文件生成唯一的ID
                myHash = hash.hexdigest()
                #打开文件夹下的一个文件
                with open(completeFileName,'rb') as fh:
                    soup = BeautifulSoup(fh,'html5lib')
                fh.close()
                #下面基本都是解包、搜索对应特征标签，提出相关内容。使用者需要根据自己页面的结构自行修改
                [m.extract() for m in soup.find_all('style')]
                titleEle = soup.find(name='h1')
                if titleEle is None:
                    continue
                contentEle = soup.find(name='div',class_='TRS_Editor')
                if contentEle is None:
                    continue
                txtContent = contentEle.get_text().replace('\n','')
                titleContent = titleEle.get_text()
                urlPrefix = 'http://www.tianjinwe.com/'
                url = completeFileName.replace(rootdir, urlPrefix)
                #这里因为是圈梁修改，所以'action'对应的永远是index，index一般是出现在要求重新索引（更新），新增内容时候所传递的action。指定hashid是处于要兼容更新、新增两种操作，对于update，有hashid能用于找到数据，对于insert new record可视作人工添加主索引数据，title是标题，content是内容，url是指在外网访问的数据的地址
                normal = json.dumps({'action':'index', 'hashid': myHash, 'title': titleContent, 'content':txtContent, 'url':  url}, ensure_ascii=False)
                result = http.request('POST', logstash, body=normal.encode('utf-8'), headers={'Content-Type': 'application/json'})
                if result.status == 200:
                    #print(normal)
                    _counter += 1
                    print("success:", myHash)
                    if _counter % 10000 == 0:
                      print("_counter", _counter)
                else:
                    print("fail:", myHash, ":", result.status,":", completeFileName)
        
        ```